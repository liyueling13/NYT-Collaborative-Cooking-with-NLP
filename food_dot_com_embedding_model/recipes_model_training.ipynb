{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis via food.com Word Embedding\n",
    "In this notebook, we load in a word embedding and sentiment analyzer which are both trained specifically on food.com reviews to predict sentiments on reviews for the NYT Salted Tahini Chocolate Chip Cookies recipe (https://cooking.nytimes.com/recipes/1018055-salted-tahini-chocolate-chip-cookies). Our hypothesis is that this would produce a better accuracy that using a generic word embedding and/or sentiment analyzer. \n",
    "\n",
    "The data set used to train these models can be found here: https://www.kaggle.com/irkaal/foodcom-recipes-and-reviews\n",
    "\n",
    "This notebook is part of a collaborative project completed for The Erdos Institute's Code 2021 Data Science Boot camp. The work in this notebook was completed by Anila Yadavalli. The other teammates are Shirley Li, Nida Obtake, and Enkhzaya Enkhtaivan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IT_pnlL5QoM7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anila\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict \n",
    "\n",
    "import spacy \n",
    "import numpy as np\n",
    "import gensim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reading in the .csv file which contains the cleaned reviews from the food.com dataset as a dataframe. This is so that we can train the sentiment analyzer later. \n",
    "\n",
    "The cleaning consists of these items:\n",
    "\n",
    "-Removes non-alphabetic characters.\n",
    "\n",
    "-Lemmatizes the words (i.e. 'ran', 'run', 'running', 'runs' all become 'run')\n",
    "\n",
    "-Creates bigrams of common words that appear together (i.e. 'chocolate chip' becomes 'chocolate_chip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "L1o3e_AqRB7H",
    "outputId": "3c6368da-0b40-4c34-a332-54ec2bebe670",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>DateSubmitted</th>\n",
       "      <th>DateModified</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>2008</td>\n",
       "      <td>gayg msft</td>\n",
       "      <td>5</td>\n",
       "      <td>better than any you can get at a restaurant!</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>well than any you can get at a restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4384</td>\n",
       "      <td>1634</td>\n",
       "      <td>Bill Hilbrich</td>\n",
       "      <td>4</td>\n",
       "      <td>I cut back on the mayo, and made up the differ...</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>I cut back on the mayo and make up the differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4523</td>\n",
       "      <td>2046</td>\n",
       "      <td>Gay Gilmore ckpt</td>\n",
       "      <td>2</td>\n",
       "      <td>i think i did something wrong because i could ...</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>I think I do something wrong because I could t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7435</td>\n",
       "      <td>1773</td>\n",
       "      <td>Malarkey Test</td>\n",
       "      <td>5</td>\n",
       "      <td>easily the best i have ever had.  juicy flavor...</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>easily the good I have ever have juicy flavorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>2085</td>\n",
       "      <td>Tony Small</td>\n",
       "      <td>5</td>\n",
       "      <td>An excellent dish.</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>an excellent dish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewId  RecipeId  AuthorId        AuthorName  Rating  \\\n",
       "0         2       992      2008         gayg msft       5   \n",
       "1         7      4384      1634     Bill Hilbrich       4   \n",
       "2         9      4523      2046  Gay Gilmore ckpt       2   \n",
       "3        13      7435      1773     Malarkey Test       5   \n",
       "4        14        44      2085        Tony Small       5   \n",
       "\n",
       "                                              Review         DateSubmitted  \\\n",
       "0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n",
       "1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n",
       "2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n",
       "3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n",
       "4                                 An excellent dish.  2000-03-28T12:51:00Z   \n",
       "\n",
       "           DateModified                                              clean  \n",
       "0  2000-01-25T21:44:00Z          well than any you can get at a restaurant  \n",
       "1  2001-10-17T16:49:59Z  I cut back on the mayo and make up the differe...  \n",
       "2  2000-02-25T09:00:00Z  I think I do something wrong because I could t...  \n",
       "3  2000-03-13T21:15:00Z  easily the good I have ever have juicy flavorf...  \n",
       "4  2000-03-28T12:51:00Z                                  an excellent dish  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_reviews_with_ratings_and_stops.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove any reviews that have a 0-star rating. This is because 0-stars indicate that the reviewer did not select a star rating, so the sentiment for such reviews cannot be classified. This removes about 80,000 reviews from the food.com dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401768\n",
      "1325520\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df[df.Rating != 0].reset_index(drop = True)\n",
    "df.sample(20)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assign a numeric score to the sentiment of a review based on the star-rating. Ratings of 4 and 5 are classified as positive and ratings of 1-3 are classified as negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "VGdm0taVRYXn",
    "outputId": "8cc7c2a2-79be-4d3f-8712-d7e0cf60d176",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>DateSubmitted</th>\n",
       "      <th>DateModified</th>\n",
       "      <th>clean</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>2008</td>\n",
       "      <td>gayg msft</td>\n",
       "      <td>5</td>\n",
       "      <td>better than any you can get at a restaurant!</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>well than any you can get at a restaurant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4384</td>\n",
       "      <td>1634</td>\n",
       "      <td>Bill Hilbrich</td>\n",
       "      <td>4</td>\n",
       "      <td>I cut back on the mayo, and made up the differ...</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>I cut back on the mayo and make up the differe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4523</td>\n",
       "      <td>2046</td>\n",
       "      <td>Gay Gilmore ckpt</td>\n",
       "      <td>2</td>\n",
       "      <td>i think i did something wrong because i could ...</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>I think I do something wrong because I could t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7435</td>\n",
       "      <td>1773</td>\n",
       "      <td>Malarkey Test</td>\n",
       "      <td>5</td>\n",
       "      <td>easily the best i have ever had.  juicy flavor...</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>easily the good I have ever have juicy flavorf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>2085</td>\n",
       "      <td>Tony Small</td>\n",
       "      <td>5</td>\n",
       "      <td>An excellent dish.</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>an excellent dish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewId  RecipeId  AuthorId        AuthorName  Rating  \\\n",
       "0         2       992      2008         gayg msft       5   \n",
       "1         7      4384      1634     Bill Hilbrich       4   \n",
       "2         9      4523      2046  Gay Gilmore ckpt       2   \n",
       "3        13      7435      1773     Malarkey Test       5   \n",
       "4        14        44      2085        Tony Small       5   \n",
       "\n",
       "                                              Review         DateSubmitted  \\\n",
       "0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n",
       "1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n",
       "2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n",
       "3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n",
       "4                                 An excellent dish.  2000-03-28T12:51:00Z   \n",
       "\n",
       "           DateModified                                              clean  \\\n",
       "0  2000-01-25T21:44:00Z          well than any you can get at a restaurant   \n",
       "1  2001-10-17T16:49:59Z  I cut back on the mayo and make up the differe...   \n",
       "2  2000-02-25T09:00:00Z  I think I do something wrong because I could t...   \n",
       "3  2000-03-13T21:15:00Z  easily the good I have ever have juicy flavorf...   \n",
       "4  2000-03-28T12:51:00Z                                  an excellent dish   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'] = (df['Rating'] > 3).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning generated empty values for some reviews. We drop those here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fnRFsUp-Ut8q"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our word embedding which was trained on the food.com data. See https://colab.research.google.com/drive/1uqw557Y0l4dOIxO_jTZHUX6Zec9T7Pkl?usp=sharing\n",
    "for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7lkq-FERVTeb"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# have to change the path if connecting to Google Drive.\n",
    "model = Word2Vec.load(\"recipes2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we need to create an array whose columns consist of word vectors for each review, so we need to know the length of the longest review. Alternately, you just pick a maximum length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "876490e49edc40b0ac1708c8884e2141",
      "291edd4c87004b8d9f4f38748092d29e",
      "1081862a6b3746cd8988c806dfc05638",
      "1749ec70f2c64a60820de81414795ba5",
      "340ef84c2dc745beb6c9b7a8b350c2c8",
      "fbdf70f57c7641ce8fbcc16b69a54ebb",
      "b143fef7d3c441beaa2f07746e7bbb2e",
      "bc6709e859c94a459f2c592304e4ea4a"
     ]
    },
    "id": "1fkWmYg9W6oc",
    "outputId": "01b381f5-4159-4687-c5e7-5bd7706c8d86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99895794290f474882d719c60c7db59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max: 1276 index: 1213989\n"
     ]
    }
   ],
   "source": [
    "# find max sentence length\n",
    "max_val = 0\n",
    "idx = 0\n",
    "for i, review in tqdm(enumerate(df['clean'])):\n",
    "      if len(review.split(' ')) > max_val:\n",
    "        max_val = len(review.split(' '))\n",
    "        idx = i\n",
    "print(\"max:\", max_val, \"index:\", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that reads in a review and produces a matrix whose columns are the word vector for each word in the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H2YO7BZpa8Xi"
   },
   "outputs": [],
   "source": [
    "def sentence_to_matrix(sentence, maxlen = max_val, model = model):\n",
    "  #takes in a sentence as a string and outputs a matrix whose rows\n",
    "  #are word vectors for each word in the sentence\n",
    "    sentence_matrix = np.zeros((maxlen, 300))\n",
    "    \n",
    "    #Split the input sentence into words.\n",
    "    sen_len = len(sentence.split(' '))\n",
    "    shift = 0\n",
    "    sen = sentence.split(' ')\n",
    "    \n",
    "    # two loops depending on if the sentence is past the length of maxlen\n",
    "    if sen_len > maxlen:\n",
    "        for i in range(maxlen):\n",
    "            # here sen[::-1] is the reversed list of sen\n",
    "            # we populate the word vector from the back because it needs to be front-padded with zeros\n",
    "            # (otherwise it will just erase everything it learned)\n",
    "            if sen[::-1][i] in model.wv.index_to_key:\n",
    "                sentence_matrix[maxlen - (i+1) + shift,:] = model.wv[sen[::-1][i]]\n",
    "            else:\n",
    "                shift += 1 #skip any words that aren't in the dictionary\n",
    "    else:\n",
    "        for i in range(sen_len):\n",
    "            if sen[::-1][i] in model.wv.index_to_key: \n",
    "                sentence_matrix[maxlen - (i+1) + shift,:] = model.wv[sen[::-1][i]]\n",
    "            else:\n",
    "                shift += 1\n",
    "    \n",
    "    return sentence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the sentiment analyzer! The first step is to create a validation data frame; we chose to use a balanced set of 5-star and 1-star reviews. We used the most extreme ratings to ensure that the sentiment analyzer to make sure that the data was actually positive or negative. Reviews in the 2-4 range could be more neutral. \n",
    "\n",
    "Alternately, one could use 'Sentiment' == 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1315921 1313921 2000\n"
     ]
    }
   ],
   "source": [
    "# create validation set of 1000 positive, 1000 negative\n",
    "df_positive = df.loc[df['Rating']==5]\n",
    "df_negative = df.loc[df['Rating']==1]\n",
    "df_val = pd.concat([df_negative.sample(1000), df_positive.sample(1000)])\n",
    "\n",
    "# ensures training set and validation set are disjoint\n",
    "df_train = df.drop(df_val.index)\n",
    "print(len(df), len(df_train), len(df_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to create training sets that are balanced between negative and positive reviews. We do this in batches of 1000, otherwise the it is too big for a laptop to handle. \n",
    "\n",
    "We create a balanced data set so that the model doesn't 'learn' to just predict positive every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample with 1000 positive reviews and 1000 negative reviews\n",
    "def get_sample(df, size=1000):\n",
    "    X_train = np.zeros((size*2, max_val, 300))\n",
    "    y_train = np.zeros(size*2)\n",
    "    # Could use 'Sentiment' = 0 or 1 instead\n",
    "    df_positive = df.loc[df['Sentiment']==1]\n",
    "    df_negative = df.loc[df['Sentiment']==0]\n",
    "    #creates a data frame containing 50% positive and 50% negative reviews\n",
    "    df_small = pd.concat([df_negative.sample(size), df_positive.sample(size)])\n",
    "    for i in range(len(df_small)):\n",
    "        row = df_small.iloc[i] #loops through df_small and takes out each row as a dictionary\n",
    "        X_train[i] = sentence_to_matrix(row['clean'])\n",
    "        y_train[i] = row['Sentiment']\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_SZSouL_fC6-"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the recurrent neural network (RNN) to train. RNN is a model designed to work with sequence data, which is why can use it to pass in sequences of word vectors (i.e. one review). \n",
    "\n",
    "The output of the model is a single value between 0 and 1 which tells you the probability of a review being positive. \n",
    "\n",
    "We added L2 regularization to the RNN in order to limit the size of the parameters, to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wpHuwrcPfRCg"
   },
   "outputs": [],
   "source": [
    "# added in regularization (L2 like ridge regression!)\n",
    "# Regularizers encourage parameters to stay small using L2 norm.\n",
    "# This just prevents overfitting.\n",
    "\n",
    "RNN = models.Sequential()\n",
    "RNN.add(layers.SimpleRNN(300,\n",
    "                         kernel_regularizer=regularizers.L2(0.01),\n",
    "                         bias_regularizer=regularizers.L2(0.01),\n",
    "                         recurrent_regularizer=regularizers.L2(0.01),\n",
    "                         return_sequences=False))\n",
    "RNN.add(layers.Dense(1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer ```rmsprop``` is a standard optimization algorithm. \n",
    "The loss function ```binary_crossentropy``` is specific to binary classification (e.g. positive/negative)\n",
    "We use ```accuracy``` as a metric to evaluate our model during training. Alternately, you could use ```AUC```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k2c3xs-_fvY7"
   },
   "outputs": [],
   "source": [
    "RNN.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn our validation data frame into sentence matrices that we  can input into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = get_sample(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we are actually training the model. For better accuracy, change ```range(6)``` to a higher number, but 6 is the maximum I could do on my laptop without it crashing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLvBILFGhKUv",
    "outputId": "508ccc15-051c-44a1-c711-d0ff12adbf83"
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    X_train, y_train = get_sample(df_train)\n",
    "    RNN.fit(X_train, y_train,\n",
    "                  epochs = 1,\n",
    "                  batch_size = 128,\n",
    "               validation_data=(X_val,y_val))\n",
    "    #print(RNN.predict(X_val[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model is trained, we can save it for future use. \n",
    "RNN.save('rnnmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model on NYT reviews\n",
    "Now we can use this model to predict sentiments on NYT reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "RNN = models.load_model('rnnmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmk</td>\n",
       "      <td>Yum.  These took much longer than 16 minutes t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>yum take long minute cook denver ft altitude g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sonya</td>\n",
       "      <td>If you follow the recipe as written the tahini...</td>\n",
       "      <td>pos</td>\n",
       "      <td>follow recipe write tahini sesame flavour cook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KV</td>\n",
       "      <td>I have made these cookies 5 times. My advice i...</td>\n",
       "      <td>pos</td>\n",
       "      <td>cookie time advice recipe say don t tell step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MaryN</td>\n",
       "      <td>I liked this- the tahini is slightly more subt...</td>\n",
       "      <td>pos</td>\n",
       "      <td>like tahini slightly subtle pb cookie combine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maggie B</td>\n",
       "      <td>Used Shaila M's tweaks. Baked first tray strai...</td>\n",
       "      <td>pos</td>\n",
       "      <td>shaila m tweak bake tray straight mix deliciou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user                                            comment sentiment  \\\n",
       "0       lmk  Yum.  These took much longer than 16 minutes t...       pos   \n",
       "1     Sonya  If you follow the recipe as written the tahini...       pos   \n",
       "2        KV  I have made these cookies 5 times. My advice i...       pos   \n",
       "3     MaryN  I liked this- the tahini is slightly more subt...       pos   \n",
       "4  Maggie B  Used Shaila M's tweaks. Baked first tray strai...       pos   \n",
       "\n",
       "                                               clean  \n",
       "0  yum take long minute cook denver ft altitude g...  \n",
       "1  follow recipe write tahini sesame flavour cook...  \n",
       "2  cookie time advice recipe say don t tell step ...  \n",
       "3  like tahini slightly subtle pb cookie combine ...  \n",
       "4  shaila m tweak bake tray straight mix deliciou...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the cleaned tahini dataset here\n",
    "tahini = pd.read_csv(\"tahini_cleaned_comments.csv\")\n",
    "tahini.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, since we are testing our model, we assigned sentiments to the NYT reviews manually. Here we are assigning a Sentiment_Score of 0 (negative) or 1 (positive). Leaving neutral comments in, and classifying them as negative was agreed upon for comparison in the context of the overall project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tahini['Sentiment_Score'] = (tahini['sentiment'] == 'pos').astype(int)\n",
    "tahini.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921804eb3c614f799b09bb455e384846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max: 57 index: 170\n"
     ]
    }
   ],
   "source": [
    "# Use sentence_to_matrix to convert NYT reviews to vectors\n",
    "# Replace 'tahini' with 'tahini_no_neu' if you decided to leave out neutrals above. \n",
    "\n",
    "tahini_size = len(tahini)\n",
    "tahini_max_val = 0\n",
    "tahini_idx = 0\n",
    "\n",
    "for i, review in tqdm(enumerate(tahini['clean'])):\n",
    "      if len(review.split(' ')) > tahini_max_val:\n",
    "        tahini_max_val = len(review.split(' '))\n",
    "        tahini_idx = i\n",
    "print(\"max:\", tahini_max_val, \"index:\", tahini_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are turning the ```tahini``` data frame into a matrix so we can input it into ```RNN```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((tahini_size, tahini_max_val, 300))\n",
    "y_test = np.zeros(tahini_size)\n",
    "\n",
    "for i in range(len(tahini)):\n",
    "    row = tahini.iloc[i] #loops through tahini and takes out each row as a dictionary\n",
    "    X_test[i] = sentence_to_matrix(row['clean'],tahini_max_val)\n",
    "    y_test[i] = row['Sentiment_Score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1276, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1276, 300), dtype=tf.float32, name='simple_rnn_input'), name='simple_rnn_input', description=\"created by layer 'simple_rnn_input'\"), but it was called on an input with incompatible shape (None, 57, 300).\n"
     ]
    }
   ],
   "source": [
    "# do RNN.predict to see the prediction!\n",
    "preds = RNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(positive) =  [0.0262641] ; Actual Sentiment =  1 \n",
      " Actual comment:  Yum.  These took much longer than 16 minutes to cook - but I'm also in Denver (5000+ ft. altitude) and used a gluten free flour blend.  Either or both of those could have affected the outcome.\n",
      "\n",
      "P(positive) =  [0.4652537] ; Actual Sentiment =  1 \n",
      " Actual comment:  If you follow the recipe as written the tahini (sesame) flavour in the cookies is VERY subtle. If I didn't know it was there, I wouldn't have tasted it- I got the same feedback from the people I shared the cookies with. That being said they are still absolutely delicious cookies...\n",
      "\n",
      "P(positive) =  [0.12418267] ; Actual Sentiment =  1 \n",
      " Actual comment:  I have made these cookies 5 times. My advice is do everything the recipe says. Don’t tell your step-father you used tahini and he will tell the world they are the best cookies he ever had. Fantastic. I wish someone had told me earlier in life about refrigerating dough. For me it made the whole experience more relaxing. Make the dough. Put in fridge. Clean up. Make cookies next day. Eat them and don’t have any dishes.\n",
      "\n",
      "P(positive) =  [0.34041995] ; Actual Sentiment =  1 \n",
      " Actual comment:  I liked this- the tahini is slightly more subtle than a PB cookie. Combined dark chocolate and milk chocolate chips, and used a sprinkle of kosher salt- did a little test, sprinkled when raw, and just out of of the oven. No difference. Also added a tsp of cornstarch. Doubled it and froze the balls of dough- baked for more like 11-13  minutes.\n",
      "\n",
      "P(positive) =  [0.77811533] ; Actual Sentiment =  1 \n",
      " Actual comment:  Used Shaila M's tweaks. Baked first tray straight after mixing-- delicious but very crispy cookies resulted. They were a little less crispy when baked after refrigeration. The best though is to bake a frozen dough ball, then the cookie retains a lot more height, and softness inside. Very tasty.\n",
      "\n",
      "P(positive) =  [0.59276485] ; Actual Sentiment =  1 \n",
      " Actual comment:  Handled these like refrigerator cookies.  Formed into long roll 1-1/4 inch in diameter.  Wrapped in wax paper. After refrigerating overnight, popped into freezer for a few minutes.  Cut roll in wax paper into thirds, took out one at a time.  Cut into 1/4 inch slices, after each cut, turn roll one quarter of a circle or 90 degrees to retain cylindrical shape.  On pan, flattened dough if necessary to make crisp cookie.  Baked 15 minutes until brown at edges.  Regret I forgot salt, great anyway.\n",
      "\n",
      "P(positive) =  [0.03944263] ; Actual Sentiment =  0 \n",
      " Actual comment:  I made these exactly as the recipe stated, but they turned out really crisp and dry rather than chewy. Not sure if I did something wrong or if that's the recipe!\n",
      "\n",
      "P(positive) =  [0.3155997] ; Actual Sentiment =  1 \n",
      " Actual comment:  I made them for Passover and used matzo cake meal. I removed 1 TBS. from the cup to compensate.  When I baked them I used convection bake at 325 degrees but found 20 minutes was my 'sweet spot'. I don't like soft cookies and the extra time was perfect. I baked the 1st batch at 325 degrees but they were not done to my taste, the 'convection' was just enough extra.\n",
      "\n",
      "P(positive) =  [0.17865881] ; Actual Sentiment =  1 \n",
      " Actual comment:  I accidentally made these with 4T of butter instead of 4oz (where 4T is only 2oz) and they turned out wonderfully!! Surprisingly squishier than usual, and the tahini flavor was strong. I might have to cut the butter in half again the next time I make them.\n",
      "\n",
      "P(positive) =  [0.36442178] ; Actual Sentiment =  1 \n",
      " Actual comment:  I have yet to meet someone that doesn’t love this cookie.\n",
      "We make them as the recipe states using TJs chocolate chunks.\n",
      "\n",
      "We use a Tb or 2 Tb scoop (I like smaller cookies, daughter likes bigger cookies)\n",
      "\n",
      "We make these a few times a year.\n",
      "\n",
      "P(positive) =  [0.2963624] ; Actual Sentiment =  0 \n",
      " Actual comment:  Try with half brown sugar\n",
      "\n",
      "P(positive) =  [0.26261276] ; Actual Sentiment =  1 \n",
      " Actual comment:  I forgot to add the eggs until after I had mixed in the flour and chips so I incorporated them with my hands.   Hilled dough just two hours. Cookies were delicious despite my mistakes which tells me the recipe is solid and forgiving. My cookies has a slightly sandy texture which was quite nice.\n",
      "\n",
      "P(positive) =  [0.11804277] ; Actual Sentiment =  0 \n",
      " Actual comment:  I wasn't impressed, but I might have used \"too good\" chocolate chips (Guittard 63%). I followed the recipe to the letter because that's how I like to do it the first time around. I found myself wanting more of the tahini flavor to come through. As is, these were just so-so chocolate chip cookies to me. If I do make again, I will use semisweet chips, and increase the tahini. My expectations were high, too, because I am a big fan of the Peanut Butter-Miso cookies. Not even in the same league, imo.\n",
      "\n",
      "P(positive) =  [0.78215563] ; Actual Sentiment =  1 \n",
      " Actual comment:  I just made this recipe to the t and they came out sublime. The tahini wasn't super strong, but the cookies are soft and delicious and the salt is a fantastic finish. Wouldn't change a thing.\n",
      "\n",
      "P(positive) =  [0.06231835] ; Actual Sentiment =  1 \n",
      " Actual comment:  This recipe is fantastic on its own, but also does well with pistachio butter (in lieu of tahini) and a dash of cardamom for the pantry pastry chef looking to please her sweet tooth on whim.\n",
      "\n",
      "P(positive) =  [0.25335902] ; Actual Sentiment =  1 \n",
      " Actual comment:  I can't say these were the best choc chip cookies I've ever had, but still a solid cookie. I would try to up the tahini flavor if possible, or maybe split the sugar half-half between white and brown sugar. They were a bit lacking in depth of flavor. But still pretty tasty!\n",
      "\n",
      "P(positive) =  [0.1061635] ; Actual Sentiment =  1 \n",
      " Actual comment:  We love these cookies and have baked them many times.  Most recently we used King Arthur Paleo flour and stevia chocolate chips to very good effect.  The flour didn’t seem to make any difference at all, and independent tasters agreed that the stevia chocolate chips paired quite well with the dough to reduce the sugar content.\n",
      "\n",
      "P(positive) =  [0.3809238] ; Actual Sentiment =  0 \n",
      " Actual comment:  For me, this took much longer to bake than it says and the cookies REALLY expand.\n",
      "\n",
      "P(positive) =  [0.31069297] ; Actual Sentiment =  1 \n",
      " Actual comment:  Used a cheap electric hand mixer for these, and the dough ended up very light, fluffy, and airy! Let dough sit in freezer for 1hr before baking. \n",
      "Made vegan with plant based butter and 2 flax eggs (1TBSP flax meal + 3 TBSP water x 2). Used a commenter's suggestion of 1/2 cup white sugar and 1/2 cup light brown sugar. \n",
      "Cookies ended up slightly flat (as in photo/video), but chewy and very light and crispy :)\n",
      "\n",
      "P(positive) =  [0.8729174] ; Actual Sentiment =  1 \n",
      " Actual comment:  I always reduce the sugar to 1/2 cup - delicious!\n",
      "\n",
      "P(positive) =  [0.04401237] ; Actual Sentiment =  1 \n",
      " Actual comment:  A perfectly fine cookie, but I expected more of the tahini to come through. Listen to other posters who said they upped it.\n",
      "\n",
      "P(positive) =  [0.3457719] ; Actual Sentiment =  0 \n",
      " Actual comment:  Add more tahini and less chocolate chips\n",
      "Try some time with some rye flour\n",
      "\n",
      "P(positive) =  [0.02659777] ; Actual Sentiment =  0 \n",
      " Actual comment:  modifications:\n",
      "-used just 1/2 cup demerara sugar plus about 2 Tbsp granulated\n",
      "-1 large egg\n",
      "-used table salt by accident so omitted top sprinkle.\n",
      "\n",
      "Cookies were good. Sweet enough but they did not spread.  Taste was a bit too buttery. Sort of sandy texture (from demerara?) but tahini flavor was subdued. Was not blown away but I did mess with the formula. Will probably play around some more with this.\n",
      "\n",
      "P(positive) =  [0.9301654] ; Actual Sentiment =  1 \n",
      " Actual comment:  Made exactly as recipe instructs and they are wonderful. My coworkers request this recipe specifically when I’m in a baking mood. Delicious!\n",
      "\n",
      "P(positive) =  [0.07641596] ; Actual Sentiment =  1 \n",
      " Actual comment:  I have made these cookies twice, following slight tweaks mentioned by Shaila M (more tahini, more chocolate, smaller cookies, 2 eggs ...) and each time when someone had them, they said it was the best cookie they have ever had. Unprompted!\n",
      "\n",
      "P(positive) =  [0.6863042] ; Actual Sentiment =  1 \n",
      " Actual comment:  The tahini really makes the cookies ver good. I used sesame tahini and used bittersweet chocolate chips. The cookies have a great flavor and taste very rich. I like how the recipe includes sea salt on top of the cookies. I just made them and they taste unbelievably delicious!\n",
      "\n",
      "P(positive) =  [0.9308419] ; Actual Sentiment =  1 \n",
      " Actual comment:  I made these with matzoh meal (regular, not cake, as I didn’t initially realize there is a difference) because I had just the right amount on hand and I wanted to use it up. I also used 2 whole eggs because I didn’t want to deal with the leftover white. And I chopped up some bars of valrhona chocolate. Definitely let them chill overnight as the dough was very soft. They are fantastic!\n",
      "\n",
      "P(positive) =  [0.8736516] ; Actual Sentiment =  1 \n",
      " Actual comment:  Froze the dough for 1 hour before baking, skipped the added Maldon salt, and layered with vanilla ice cream for a decadent dessert. Turned out perfectly!\n",
      "\n",
      "P(positive) =  [0.05299273] ; Actual Sentiment =  0 \n",
      " Actual comment:  Has anyone had any success making this with a food processor instead of a stand mixer? I made the dough with a processor tonight and it didn't really get light and fluffy— felt like more of a nougat. And the dough was hot from being processed, so the chips melted a bit when I put them in. Wondering if anyone has advice, or are food processors just not as suitable for cookie dough as their manuals claim them to be?\n",
      "\n",
      "P(positive) =  [0.5330083] ; Actual Sentiment =  1 \n",
      " Actual comment:  This recipe is incredible! I made these cookies for the holidays and gave them as gifts. I also kept a bunch for myself, of course. I substituted Cup 4 Cup gluten free flour and the results were chewy and perfect.\n",
      "\n",
      "P(positive) =  [0.2121824] ; Actual Sentiment =  1 \n",
      " Actual comment:  Made the dough last night and baked a batch this morning. Only change to the recipe was that I used slightly less chocolate chunks due to the size of the package had only 200g. Result was delicious!\n",
      "\n",
      "I like that I’ve still got cookie dough in the fridge to bake a few more fresh cookies at a later date. Used a 2” scoop, and was able to fit 6 on the cookie sheet. The cookies expanded to 4 - 5 inches diameter once baked.\n",
      "\n",
      "Next time will try with 2 whole eggs (or just 1) as others have suggested.\n",
      "\n",
      "P(positive) =  [0.2780071] ; Actual Sentiment =  0 \n",
      " Actual comment:  See Brenda's notes in Pages document\n",
      "\n",
      "P(positive) =  [0.8912209] ; Actual Sentiment =  1 \n",
      " Actual comment:  Excellent cookie\n",
      "Make exactly as recipe.....made 4 dozen using cookie scoop, which is a nice manageable size for eating.\n",
      "\n",
      "P(positive) =  [0.9683292] ; Actual Sentiment =  1 \n",
      " Actual comment:  Made this with half brown sugar and half granulated sugar. Browned the butter. Also added walnuts and halved the amount of chocolate chips because I was out. It came out delicious!\n",
      "\n",
      "P(positive) =  [0.6142199] ; Actual Sentiment =  0 \n",
      " Actual comment:  FREEZE DOUGH BALLS ~ 5 min prebake to prevent spread. For double batch (got ~ 80 cookies) used 4 whole eggs, 1.25 cup tahini, scant 2 t kosher salt & most of 2 bags choc chips (1 bag Tollhouse semisweet chunks, 1 bag Tollhouse dark chips) - saving a bit of each type to top the cookie dough balls before baking. Topped w Maldon right out of oven. Next time up tahini even more (1.5 cups for double batch?) & use a tad less choc chips (1.5 bags total for double batch?).\n",
      "\n",
      "P(positive) =  [0.72332215] ; Actual Sentiment =  1 \n",
      " Actual comment:  Wow, don’t change a thing except more chocolate chips.  These are absolutely amazing cookies.\n",
      "\n",
      "P(positive) =  [0.01847029] ; Actual Sentiment =  1 \n",
      " Actual comment:  These cookies are always a huge hit EVERY time I make them, which has been at least 2 dozen times. I've adapted the recipe to be gluten free but the most G-free suspects devour them. To make G-free, I use a 1/3 cup almond flour, 1/3 cup coconut flour, and 1/3 (+ 2 TB) of the Bob Red Mill Gluten Free flour mix. Also, I cut the sugar to 2/3 cup. Nobody misses the gluten or sugar.\n",
      "\n",
      "P(positive) =  [0.07028389] ; Actual Sentiment =  0 \n",
      " Actual comment:  Made more or less per recipe (subbed brown sugar and threw some cacao nibs in) and was disappointed. Tahini doesn't come through at all. \n",
      "\n",
      "It's not a bad recipe, it's just not what I want. I was hoping for the chewy and slightly crystallized texture and sesame flavor of hair halvah in a cookie with the benefit of chocolate. Instead, this is a cakey & greasy plain chocolate chip cookie.\n",
      "\n",
      "P(positive) =  [0.9744588] ; Actual Sentiment =  1 \n",
      " Actual comment:  Just made these and thought they were great. I would add more tahini next and I did cook slightly smaller cookies for about 20 minutes. They are moist and crispy in just the right doses.\n",
      "A keeper!\n",
      "\n",
      "P(positive) =  [0.76949644] ; Actual Sentiment =  1 \n",
      " Actual comment:  I subbed Trader Joe’s GF flour blend (no xanthan gum). Also subbed in maple syrup for vanilla. Did everything else as described, including refrigeration and they turned out great!\n",
      "\n",
      "P(positive) =  [0.50904465] ; Actual Sentiment =  1 \n",
      " Actual comment:  I’ve made these dozens of times; just tried with black sesame and it yielded amazing jet black cookies that look burnt but taste wonderful.\n",
      "\n",
      "P(positive) =  [0.07638592] ; Actual Sentiment =  0 \n",
      " Actual comment:  Followed instructions perfectly.  Made 12 cookies.  Took twice as long to cook. Maybe because dough came from fridge, but I didn’t see anything say to let dough come to room before cooking.  These are enormous cookies.  No reason you can’t make 24 or more from dough, although I feel less guilt when I say I only at three.\n",
      "\n",
      "P(positive) =  [0.44785255] ; Actual Sentiment =  0 \n",
      " Actual comment:  Edit to add \n",
      "Mixed by hand.\n",
      "No problem.\n",
      "\n",
      "P(positive) =  [0.2596634] ; Actual Sentiment =  1 \n",
      " Actual comment:  Nice delicious change from usual cookies.\n",
      "Crispy and chewy.  Sweet, salty, nutty.\n",
      "Read the notes and made following changes:\n",
      "-1/2 c brown sugar   1/4 c granular sugar\n",
      "-Replaced egg with 3tbsp aquafaba (liquid from chickpeas)\n",
      "-Increased baking pwdr 1/4tsp to account for aquafaba \n",
      "-Refrigerated 2 hrs only\n",
      "-30ml scoop (1/8c) used for cookies\n",
      "-Salted before baking; slightly flattened cookie before baking.  Gave nice shape, v small dome to cookie\n",
      "-Convection bake 325F, 18-19 mins.\n",
      "30 cookies\n",
      "\n",
      "P(positive) =  [0.08344239] ; Actual Sentiment =  1 \n",
      " Actual comment:  These cookies were delicious, but the came out looking like normal cookies with a domed center.  How can I make them look like the photo?\n",
      "\n",
      "P(positive) =  [0.02686039] ; Actual Sentiment =  0 \n",
      " Actual comment:  Were anyone’s cookie dough dry? As in so dry it was hard to scoop the dough into balls? Was the end of my tahini jar, so maybe there wasn’t enough oil\n",
      "\n",
      "P(positive) =  [0.20940414] ; Actual Sentiment =  0 \n",
      " Actual comment:  325 is too low, 20 minutes and now brown on the edges.  Trying 375...a normal cookie temp.\n",
      "\n",
      "P(positive) =  [0.9360012] ; Actual Sentiment =  1 \n",
      " Actual comment:  Delicious! I let the dough refrigerate for 24hrs and made bigger cookies. They turned out amazing.\n",
      "\n",
      "P(positive) =  [0.5432431] ; Actual Sentiment =  1 \n",
      " Actual comment:  These are now my favourite chocolate chip cookie! Two caveats: I love halvah and I like soft cookies. Changed to 3/4 c. tahini and 2 eggs and could only wait 6 hours refrigeration. Otherwise the same. So good. Made 24 large cookies.\n",
      "\n",
      "P(positive) =  [0.9597838] ; Actual Sentiment =  1 \n",
      " Actual comment:  Made these tonight. Used 1/2 stick butter instead of whole stick, 1/3 cup each white sugar and brown sugar, only 1 cup of chocolate chunks (that's all I had) which I roughly chopped. Divided dough in half and added about 1/3 cup chopped toasted hazelnuts to half the dough. Baked small sized balls at 325 for 13 minutes. Forgot to sprinkle with salt but still absolutely divine!\n",
      "\n",
      "P(positive) =  [0.13163874] ; Actual Sentiment =  1 \n",
      " Actual comment:  These are delicious; they have a nice crumbly texture. I might have overcooked them because it seemed like they just weren’t ready — maybe the tahini makes them look wetter than usual? Also, I got 2 dozen from the recipe. Next time I’ll experiment with the brown sugar others suggested.\n",
      "\n",
      "P(positive) =  [0.89121675] ; Actual Sentiment =  1 \n",
      " Actual comment:  I didn’t have chocolate chips, so I borrowed an idea from another NYT recipe for Chocolate-Peanut Butter Swirl cookies and divided the batter in half, adding 3T of unsweetened cocoa powder to half of it. Then I rolled about 1T from one half together with 1T from the other half. The cookies turned out beautiful and delicious.\n",
      "\n",
      "P(positive) =  [0.13603854] ; Actual Sentiment =  1 \n",
      " Actual comment:  I didn't want to stray too far from the recipe, however I used a mix of chocolate chunks and mini chips. I refrigerated for 12+ hours, used a small cookie scoop (my partner had broken our larger one), and I did some tests with flattening vs not. The first batch spread super thin after 15-16 min at 325. The second was the same after 20 minutes. I tried baking them at 350 for 15, then back at 325 for 14 min. All too thin but tasted good. Just crispy instead of chewy and very pale/light in color.\n",
      "\n",
      "P(positive) =  [0.0516617] ; Actual Sentiment =  1 \n",
      " Actual comment:  These are perfect cookies. Only chilled for an hour or two because I was impatient. Salt generously before baking, not after, or the salt doesn’t stick.\n",
      "\n",
      "P(positive) =  [0.22861019] ; Actual Sentiment =  0 \n",
      " Actual comment:  Will this be difficult to make mixed by hand? I don't have a Kitchenaid or handheld mixer\n",
      "\n",
      "P(positive) =  [0.8998053] ; Actual Sentiment =  1 \n",
      " Actual comment:  This is the best choc chip cookie recipe of all time. Including tahini is actually genius. Have made it so many times during quarantine\n",
      "\n",
      "P(positive) =  [0.50701296] ; Actual Sentiment =  1 \n",
      " Actual comment:  I'd say the overall flavor depends heavily on the quality of your tahini, and accounts for why the comments on this recipe are so varied. The splurge on great tahini (homemade if you like) is worth it.\n",
      "\n",
      "I was too impatient to wait 12 hours, so I froze preportioned dough (1.5 Tbsp) for about 2 hours. Baked for 14 min at 325F, switching halfway from lower to top rack. Looked very pale when I took it out, but I patiently waited the 20 minutes -- texture was perfectly chewy-crisp! Will make again.\n",
      "\n",
      "P(positive) =  [0.21225733] ; Actual Sentiment =  0 \n",
      " Actual comment:  The cookie is tasty, but a whole stick of butter & 1/2 c. tahini made these overly greasy, and I would have enjoyed them more had they been a little more subdued in the fat category. Another poster said she accidentally used only half the butter yet her recipe turned out delicious.  She probably inadvertently improved the recipe.\n",
      "\n",
      "P(positive) =  [0.95231974] ; Actual Sentiment =  1 \n",
      " Actual comment:  Wow Great recipe!\n",
      "\n",
      "P(positive) =  [0.18859118] ; Actual Sentiment =  1 \n",
      " Actual comment:  Nutty, comforting, delicious cookies! Fun tip - after 12 hours in the refrigerator, I baked half the batter and rolled the other half into balls to keep in the freezer. Presto - instant homemade cookies whenever the craving calls.\n",
      "\n",
      "One modification I made was to use a 1/2 cup of sugar rather than the full 1 cup the recipe calls for. Still perfectly sweet :)\n",
      "\n",
      "P(positive) =  [0.54580665] ; Actual Sentiment =  1 \n",
      " Actual comment:  Cookies were good but the tahini flavor was not detectable (even after upping the amount to 3/4 cups).\n",
      "\n",
      "P(positive) =  [0.6816462] ; Actual Sentiment =  0 \n",
      " Actual comment:  I've tried with refrigerated dough and non-refrigerated, I didn't notice a difference. I had to cook much longer, over 20 minutes.\n",
      "\n",
      "P(positive) =  [0.04570538] ; Actual Sentiment =  1 \n",
      " Actual comment:  I'm at about 325 in an electric oven and definitely needed a solid 20 minutes before they were baked enough to spatula off the tray. So good though. Even without the sprinkled salt on top (which I forgot to add the first time! Oops!) Also I don't have a mixing bowl with a paddle attachment so just used an electric whisk for the wet ingredients and mixed wet and dry by hand. Definitely not ideal. Wouldn't recommend it. But it'll work in a pinch. Just to say, don't be deterred by lack of equipment\n",
      "\n",
      "P(positive) =  [0.04964587] ; Actual Sentiment =  1 \n",
      " Actual comment:  This recipe was really great and the cookies come out nice and thin and chewy yet crisp at the edges. I swapped out half the granulated sugar for brown sugar. Next time will omit the granulated and just do a half cup of brown. Huge hit! The tahini is a nice touch that offers some nuttiness and thickness to the dough but isn’t overpowering to the taste.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 64",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-64fe49a083f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtahini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     print(\"P(positive) = \", preds[i], \"; Actual Sentiment = \", tahini.loc[i, 'Sentiment_Score'],\n\u001b[0m\u001b[0;32m      3\u001b[0m          \"\\n Actual comment: \", tahini.loc[i, 'comment'])\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    871\u001b[0m                     \u001b[1;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    784\u001b[0m                 \u001b[1;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m                 \u001b[1;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m                 \u001b[0msection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[1;31m# We should never have a scalar section here, because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3489\u001b[0m             \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3491\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 64"
     ]
    }
   ],
   "source": [
    "for i in range(len(tahini)):\n",
    "    print(\"P(positive) = \", preds[i], \"; Actual Sentiment = \", tahini.loc[i, 'Sentiment_Score'],\n",
    "         \"\\n Actual comment: \", tahini.loc[i, 'comment'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the confusion matrix to determine how well our model did on the Tahini Cookie data. Note that looking at the accuracy score doesn't tell us the whole story since the data had way more positive reviews than negative review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 79,  32],\n",
       "       [110, 127]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = (preds > 0.5) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, the AUC (area under curve) score looks decent, which tells us that a lower threshold for predicting positive could give us better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5919540229885057, 0.6714562663929753)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_preds), roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we lower the threshold, we sacrifice our accuracy of negatives, for a better accuracy of positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67,  44],\n",
       "       [ 89, 148]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = (preds > 0.35) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the process, but drop the neutral sentiment reviews. We see this improves the AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0257de21d44a8984b1e8da5afce974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max: 57 index: 133\n"
     ]
    }
   ],
   "source": [
    "tahini_no_neu = tahini[tahini.sentiment != 'neu'].reset_index(drop = True)\n",
    "\n",
    "tahini_size = len(tahini_no_neu)\n",
    "tahini_max_val = 0\n",
    "tahini_idx = 0\n",
    "\n",
    "for i, review in tqdm(enumerate(tahini_no_neu['clean'])):\n",
    "      if len(review.split(' ')) > tahini_max_val:\n",
    "        tahini_max_val = len(review.split(' '))\n",
    "        tahini_idx = i\n",
    "print(\"max:\", tahini_max_val, \"index:\", tahini_idx)\n",
    "\n",
    "X_test = np.zeros((tahini_size, tahini_max_val, 300))\n",
    "y_test = np.zeros(tahini_size)\n",
    "\n",
    "for i in range(len(tahini_no_neu)):\n",
    "    row = tahini_no_neu.iloc[i] #loops through tahini and takes out each row as a dictionary\n",
    "    X_test[i] = sentence_to_matrix(row['clean'],tahini_max_val)\n",
    "    y_test[i] = row['Sentiment_Score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the confusion matrix tells us that the model predicts negative reviews very well, but there are a lot of false positives.\n",
    "\n",
    "This could mean that according to the model, a user has to really love the cookie recipe for their review to be detected as positive! 🤷🏽‍♀️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24,   2],\n",
       "       [110, 127]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = RNN.predict(X_test)\n",
    "\n",
    "y_preds = (preds > 0.5) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score looks much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5741444866920152, 0.8377150275884453)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_preds), roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like above, when we lower the threshold, we sacrifice our accuracy of negatives, for a better accuracy of positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 23,   3],\n",
       "       [ 89, 148]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = (preds > 0.35) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "recipes_model_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1081862a6b3746cd8988c806dfc05638": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbdf70f57c7641ce8fbcc16b69a54ebb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_340ef84c2dc745beb6c9b7a8b350c2c8",
      "value": 1
     }
    },
    "1749ec70f2c64a60820de81414795ba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc6709e859c94a459f2c592304e4ea4a",
      "placeholder": "​",
      "style": "IPY_MODEL_b143fef7d3c441beaa2f07746e7bbb2e",
      "value": " 1376004/? [00:09&lt;00:00, 149806.12it/s]"
     }
    },
    "291edd4c87004b8d9f4f38748092d29e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "340ef84c2dc745beb6c9b7a8b350c2c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "876490e49edc40b0ac1708c8884e2141": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1081862a6b3746cd8988c806dfc05638",
       "IPY_MODEL_1749ec70f2c64a60820de81414795ba5"
      ],
      "layout": "IPY_MODEL_291edd4c87004b8d9f4f38748092d29e"
     }
    },
    "b143fef7d3c441beaa2f07746e7bbb2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc6709e859c94a459f2c592304e4ea4a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbdf70f57c7641ce8fbcc16b69a54ebb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
